{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493e5743",
   "metadata": {},
   "source": [
    "## Welcome to this Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe1b27",
   "metadata": {},
   "source": [
    "In this notebook, we will be creating multi linear regression model from scratch and then use Iris dataset to test and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c841c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a linear regression class\n",
    "class Linear_Regression:\n",
    "    def __init__(self, X, Y): # it takes X and Y meaning whole dataset and store it \n",
    "        self.X = np.vstack((X.values.T,np.ones(X.shape[0]))).T # adding an extra column so that we can multiply as this column is for theata zero.\n",
    "        self.Y = Y.reshape(-1,1) # Reshaping Y to the 2D array which have a single column\n",
    "        self.theta = np.random.random((1,self.X.shape[1])).T # Initializing values for theta's randomly for first time\n",
    "    def hypothesis(self): # creating a function which will return hypothesis \n",
    "        h = np.dot(self.X,self.theta.reshape(-1,1))\n",
    "        #print(\"h: \",h.shape)\n",
    "        return h\n",
    "    \n",
    "    def cost_function(self): # function to calculate cost of the model\n",
    "        return np.mean((self.Y-self.hypothesis())**2)\n",
    "    \n",
    "    def der_cost_function(self): #function for derivative of cost function as it is used in updating the weights\n",
    "        dot_prod=np.dot((self.Y-self.hypothesis()).T,self.X)\n",
    "        #print(dot_prod)\n",
    "        return -2*(np.mean(dot_prod,axis=1))\n",
    "    \n",
    "    def gradient_descent_function(self,lr=0.0001): # function for calculating gradient descent \n",
    "        der = self.der_cost_function()\n",
    "        return self.theta-lr*der\n",
    "        \n",
    "    def train_function(self,no_of_it_train,no_of_it_print_cost,lr=0.0001): # Function to train the model\n",
    "        count=1 # for printing cost\n",
    "        for i in range(1,no_of_it_train):\n",
    "            gradient_val = self.gradient_descent_function(lr) # finding new weights\n",
    "            self.theta = gradient_val # updating the weights\n",
    "            if(count == no_of_it_print_cost): \n",
    "                print(\"Cost: \", self.cost_function()) # printing cost value after some number of iterations\n",
    "                count=1\n",
    "            count+=1\n",
    "    def Predict(self,X): # It is used for prediction\n",
    "        predictions= np.dot(X,self.theta)\n",
    "        return predictions\n",
    "    def get_weights(self): # Function for getting the weights\n",
    "        return self.theta\n",
    "    def printer(self): # Function to print X, Y, theta's\n",
    "        print(\"X: \",self.X)\n",
    "        print(\"Y: \",self.Y)\n",
    "        print(\"theta: \",self.theta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaf96af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width         species\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Iris.csv\") # using Iris data \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ab43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset and converting non-numarical column to numarical using LabelEncoder()\n",
    "x=df.drop('species',axis=1)\n",
    "y=df['species']\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a1779ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[5.1 3.5 1.4 0.2 1. ]\n",
      " [4.9 3.  1.4 0.2 1. ]\n",
      " [4.7 3.2 1.3 0.2 1. ]\n",
      " [4.6 3.1 1.5 0.2 1. ]\n",
      " [5.  3.6 1.4 0.2 1. ]\n",
      " [5.4 3.9 1.7 0.4 1. ]\n",
      " [4.6 3.4 1.4 0.3 1. ]\n",
      " [5.  3.4 1.5 0.2 1. ]\n",
      " [4.4 2.9 1.4 0.2 1. ]\n",
      " [4.9 3.1 1.5 0.1 1. ]\n",
      " [5.4 3.7 1.5 0.2 1. ]\n",
      " [4.8 3.4 1.6 0.2 1. ]\n",
      " [4.8 3.  1.4 0.1 1. ]\n",
      " [4.3 3.  1.1 0.1 1. ]\n",
      " [5.8 4.  1.2 0.2 1. ]\n",
      " [5.7 4.4 1.5 0.4 1. ]\n",
      " [5.4 3.9 1.3 0.4 1. ]\n",
      " [5.1 3.5 1.4 0.3 1. ]\n",
      " [5.7 3.8 1.7 0.3 1. ]\n",
      " [5.1 3.8 1.5 0.3 1. ]\n",
      " [5.4 3.4 1.7 0.2 1. ]\n",
      " [5.1 3.7 1.5 0.4 1. ]\n",
      " [4.6 3.6 1.  0.2 1. ]\n",
      " [5.1 3.3 1.7 0.5 1. ]\n",
      " [4.8 3.4 1.9 0.2 1. ]\n",
      " [5.  3.  1.6 0.2 1. ]\n",
      " [5.  3.4 1.6 0.4 1. ]\n",
      " [5.2 3.5 1.5 0.2 1. ]\n",
      " [5.2 3.4 1.4 0.2 1. ]\n",
      " [4.7 3.2 1.6 0.2 1. ]\n",
      " [4.8 3.1 1.6 0.2 1. ]\n",
      " [5.4 3.4 1.5 0.4 1. ]\n",
      " [5.2 4.1 1.5 0.1 1. ]\n",
      " [5.5 4.2 1.4 0.2 1. ]\n",
      " [4.9 3.1 1.5 0.1 1. ]\n",
      " [5.  3.2 1.2 0.2 1. ]\n",
      " [5.5 3.5 1.3 0.2 1. ]\n",
      " [4.9 3.1 1.5 0.1 1. ]\n",
      " [4.4 3.  1.3 0.2 1. ]\n",
      " [5.1 3.4 1.5 0.2 1. ]\n",
      " [5.  3.5 1.3 0.3 1. ]\n",
      " [4.5 2.3 1.3 0.3 1. ]\n",
      " [4.4 3.2 1.3 0.2 1. ]\n",
      " [5.  3.5 1.6 0.6 1. ]\n",
      " [5.1 3.8 1.9 0.4 1. ]\n",
      " [4.8 3.  1.4 0.3 1. ]\n",
      " [5.1 3.8 1.6 0.2 1. ]\n",
      " [4.6 3.2 1.4 0.2 1. ]\n",
      " [5.3 3.7 1.5 0.2 1. ]\n",
      " [5.  3.3 1.4 0.2 1. ]\n",
      " [7.  3.2 4.7 1.4 1. ]\n",
      " [6.4 3.2 4.5 1.5 1. ]\n",
      " [6.9 3.1 4.9 1.5 1. ]\n",
      " [5.5 2.3 4.  1.3 1. ]\n",
      " [6.5 2.8 4.6 1.5 1. ]\n",
      " [5.7 2.8 4.5 1.3 1. ]\n",
      " [6.3 3.3 4.7 1.6 1. ]\n",
      " [4.9 2.4 3.3 1.  1. ]\n",
      " [6.6 2.9 4.6 1.3 1. ]\n",
      " [5.2 2.7 3.9 1.4 1. ]\n",
      " [5.  2.  3.5 1.  1. ]\n",
      " [5.9 3.  4.2 1.5 1. ]\n",
      " [6.  2.2 4.  1.  1. ]\n",
      " [6.1 2.9 4.7 1.4 1. ]\n",
      " [5.6 2.9 3.6 1.3 1. ]\n",
      " [6.7 3.1 4.4 1.4 1. ]\n",
      " [5.6 3.  4.5 1.5 1. ]\n",
      " [5.8 2.7 4.1 1.  1. ]\n",
      " [6.2 2.2 4.5 1.5 1. ]\n",
      " [5.6 2.5 3.9 1.1 1. ]\n",
      " [5.9 3.2 4.8 1.8 1. ]\n",
      " [6.1 2.8 4.  1.3 1. ]\n",
      " [6.3 2.5 4.9 1.5 1. ]\n",
      " [6.1 2.8 4.7 1.2 1. ]\n",
      " [6.4 2.9 4.3 1.3 1. ]\n",
      " [6.6 3.  4.4 1.4 1. ]\n",
      " [6.8 2.8 4.8 1.4 1. ]\n",
      " [6.7 3.  5.  1.7 1. ]\n",
      " [6.  2.9 4.5 1.5 1. ]\n",
      " [5.7 2.6 3.5 1.  1. ]\n",
      " [5.5 2.4 3.8 1.1 1. ]\n",
      " [5.5 2.4 3.7 1.  1. ]\n",
      " [5.8 2.7 3.9 1.2 1. ]\n",
      " [6.  2.7 5.1 1.6 1. ]\n",
      " [5.4 3.  4.5 1.5 1. ]\n",
      " [6.  3.4 4.5 1.6 1. ]\n",
      " [6.7 3.1 4.7 1.5 1. ]\n",
      " [6.3 2.3 4.4 1.3 1. ]\n",
      " [5.6 3.  4.1 1.3 1. ]\n",
      " [5.5 2.5 4.  1.3 1. ]\n",
      " [5.5 2.6 4.4 1.2 1. ]\n",
      " [6.1 3.  4.6 1.4 1. ]\n",
      " [5.8 2.6 4.  1.2 1. ]\n",
      " [5.  2.3 3.3 1.  1. ]\n",
      " [5.6 2.7 4.2 1.3 1. ]\n",
      " [5.7 3.  4.2 1.2 1. ]\n",
      " [5.7 2.9 4.2 1.3 1. ]\n",
      " [6.2 2.9 4.3 1.3 1. ]\n",
      " [5.1 2.5 3.  1.1 1. ]\n",
      " [5.7 2.8 4.1 1.3 1. ]\n",
      " [6.3 3.3 6.  2.5 1. ]\n",
      " [5.8 2.7 5.1 1.9 1. ]\n",
      " [7.1 3.  5.9 2.1 1. ]\n",
      " [6.3 2.9 5.6 1.8 1. ]\n",
      " [6.5 3.  5.8 2.2 1. ]\n",
      " [7.6 3.  6.6 2.1 1. ]\n",
      " [4.9 2.5 4.5 1.7 1. ]\n",
      " [7.3 2.9 6.3 1.8 1. ]\n",
      " [6.7 2.5 5.8 1.8 1. ]\n",
      " [7.2 3.6 6.1 2.5 1. ]\n",
      " [6.5 3.2 5.1 2.  1. ]\n",
      " [6.4 2.7 5.3 1.9 1. ]\n",
      " [6.8 3.  5.5 2.1 1. ]\n",
      " [5.7 2.5 5.  2.  1. ]\n",
      " [5.8 2.8 5.1 2.4 1. ]\n",
      " [6.4 3.2 5.3 2.3 1. ]\n",
      " [6.5 3.  5.5 1.8 1. ]\n",
      " [7.7 3.8 6.7 2.2 1. ]\n",
      " [7.7 2.6 6.9 2.3 1. ]\n",
      " [6.  2.2 5.  1.5 1. ]\n",
      " [6.9 3.2 5.7 2.3 1. ]\n",
      " [5.6 2.8 4.9 2.  1. ]\n",
      " [7.7 2.8 6.7 2.  1. ]\n",
      " [6.3 2.7 4.9 1.8 1. ]\n",
      " [6.7 3.3 5.7 2.1 1. ]\n",
      " [7.2 3.2 6.  1.8 1. ]\n",
      " [6.2 2.8 4.8 1.8 1. ]\n",
      " [6.1 3.  4.9 1.8 1. ]\n",
      " [6.4 2.8 5.6 2.1 1. ]\n",
      " [7.2 3.  5.8 1.6 1. ]\n",
      " [7.4 2.8 6.1 1.9 1. ]\n",
      " [7.9 3.8 6.4 2.  1. ]\n",
      " [6.4 2.8 5.6 2.2 1. ]\n",
      " [6.3 2.8 5.1 1.5 1. ]\n",
      " [6.1 2.6 5.6 1.4 1. ]\n",
      " [7.7 3.  6.1 2.3 1. ]\n",
      " [6.3 3.4 5.6 2.4 1. ]\n",
      " [6.4 3.1 5.5 1.8 1. ]\n",
      " [6.  3.  4.8 1.8 1. ]\n",
      " [6.9 3.1 5.4 2.1 1. ]\n",
      " [6.7 3.1 5.6 2.4 1. ]\n",
      " [6.9 3.1 5.1 2.3 1. ]\n",
      " [5.8 2.7 5.1 1.9 1. ]\n",
      " [6.8 3.2 5.9 2.3 1. ]\n",
      " [6.7 3.3 5.7 2.5 1. ]\n",
      " [6.7 3.  5.2 2.3 1. ]\n",
      " [6.3 2.5 5.  1.9 1. ]\n",
      " [6.5 3.  5.2 2.  1. ]\n",
      " [6.2 3.4 5.4 2.3 1. ]\n",
      " [5.9 3.  5.1 1.8 1. ]]\n",
      "Y:  [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "theta:  [[0.76528612]\n",
      " [0.36859275]\n",
      " [0.11729597]\n",
      " [0.79692754]\n",
      " [0.88126972]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing our model\n",
    "lr=Linear_Regression(x,y)\n",
    "lr.printer() #checking if everthing is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e58816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n",
      "Cost:  0.8622919160315748\n"
     ]
    }
   ],
   "source": [
    "lr.train_function(10000,100) # training our model for 10000 epochs and print cost after every 100th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ade78c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.20599469])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing our model \n",
    "X_test=[5.9,3.0,5.1,1.8,1] # test example\n",
    "X_test=np.array(X_test) \n",
    "lr.test(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
